<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs_posts | Bias Barometer</title>
    <link>/blogs_posts/</link>
      <atom:link href="/blogs_posts/index.xml" rel="self" type="application/rss+xml" />
    <description>Blogs_posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 03 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/logo_hu284f5771a1ea0c89898faa204948e4a1_285145_300x300_fit_lanczos_2.png</url>
      <title>Blogs_posts</title>
      <link>/blogs_posts/</link>
    </image>
    
    <item>
      <title>The difficult problem of understanding the relation between bias in language models and human biases</title>
      <link>/blogs_posts/understanding-relation-language-model-and-human-bias/</link>
      <pubDate>Thu, 03 Feb 2022 00:00:00 +0000</pubDate>
      <guid>/blogs_posts/understanding-relation-language-model-and-human-bias/</guid>
      <description>&lt;p&gt;Artificial Intelligence (AI) is a technology that is here to stay.
In many aspects of daily life, one interacts with AI algorithms in one way or another. These algorithms range from search engines like Google, that &lt;a href=&#34;https://www.google.nl/search?q=restaurant&amp;#43;close&amp;#43;to&amp;#43;me&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;present search results in a way that hopefully is optimal to you&lt;/a&gt;, to health apps that aim to &lt;a href=&#34;https://link.springer.com/article/10.1007/s10916-021-01773-0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;support your healthy living habits&lt;/a&gt;.
Because of your omnipresent interactions with AI, it is important to understand how AI systems make decisions. This is especially true, since we need to be able to trust those decisions to be fair and &lt;a href=&#34;https://www.google.com/search?q=nurse&amp;amp;sxsrf=APq-WBu4y8ISKSVyYfHlpCP9Y_7U2MYpGA:1643629501211&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;ved=2ahUKEwiM_76E9dv1AhXL-6QKHVsoCwsQ_AUoAXoECAEQAw&amp;amp;biw=1536&amp;amp;bih=754&amp;amp;dpr=1.25&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unbiased&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A clear case where there is a danger for unfairness and bias in AI
systems is in the use of large language models. Roughly, these models
aim to capture the meaning of natural language expressions; They are
used in many domains, including automatic translation, speech
recognition, and chat bots. Obviously, it is crucial that these models
get the meaning right, and do not introduce unfair biases in the meaning
of a phrase!&lt;/p&gt;
&lt;p&gt;What makes a bias unfair? The way that most language models express the
meaning of the word &amp;ldquo;nurse&amp;rdquo; is highly associated with the way feminine
terms (woman, girl, she, etc) are expressed. This does not seem to be
what we want, since a male nurse has the same job characteristics as a
female nurse. However, this bias, and similar gender biases related to
professions, does express the &lt;a href=&#34;https://en.wikipedia.org/wiki/Gender_gap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gender
gap&lt;/a&gt;. In fact, the size of the
gender gap in terms of labor statistics over the years &lt;a href=&#34;https://www.pnas.org/content/115/16/E3635&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;has been shown
to predict&lt;/a&gt; the bias in
language over the years. Even stronger, implicit biases that humans seem
to have about (among other things) gender, &lt;a href=&#34;https://doi.org/10.1126/science.aal4230&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;are reflected in language
models&lt;/a&gt; as well. It seems thus
that to some extent language models reflect societal trends, and nothing
else. Should we hold these AI algorithms to a higher standard than we do
humans? Or should we strive to eradicate bias from language, and
consequently from human vocabulary and thought? To answer these
questions, an understanding of human bias is necessary as well.&lt;/p&gt;
&lt;p&gt;Understanding human biases comes with its own set of problems. Some
people may be more biased than others, but how do we &lt;a href=&#34;https://econtent.hogrefe.com/doi/abs/10.1027/1015-5759.24.4.210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;measure such a
difference&lt;/a&gt;?
First, it is important to realize that the question implies a golden
standard. There could be people out there that are completely unbiased!
But what would their behavior and decisions look like? This is a
normative question, that society has to address. Second, in order to
measure a difference, there need to be good measurement tools. Contrary
to many physical properties such as length or weight, there is &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S074756321100094X&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;no
precise measurement
tool&lt;/a&gt;
for human bias.&lt;/p&gt;
&lt;p&gt;One of the reasons may be that human biases and attitudes are inherently
personal, and it is extremely difficult to develop tests that allow for
the heterogeneity of biases that are induced by images, words, or names.
For example, we are currently running a study to show that biases that
have been reported in past
&lt;a href=&#34;https://implicit.harvard.edu/implicit/takeatest.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;studies&lt;/a&gt; (e.g.,
towards women) may depend on the use of specific words (as in the image
below), rather than show an implicit bias towards women per se.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/Gender-Career_IAT.png&#34; alt=&#34;&#34;&gt;
&lt;em&gt;In the Gender-Career Implicit Association Test, people have to press a
right or a left button in response to either gender names or two
supposedly gender-stereotypical categories (Career and Family). The idea
is that this is harder when there is a &amp;ldquo;mismatch&amp;rdquo; between the category
and the gender that require the same button press.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This is the challenge our research group stands for: To understand how
bias in large language models relates to bias in humans. We thus need to
address fundamental questions about the nature and measurement of such
biases. A daunting challenge for sure, but the end goal is extremely
worthwhile: To mitigate unfair biases in the AI systems we interact with
on a daily basis, such that we can gain trust in their decisions, and
fairness is achieved.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How do I know if my language model is gender-biased?</title>
      <link>/blogs_posts/gender-bias-in-language-models/</link>
      <pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate>
      <guid>/blogs_posts/gender-bias-in-language-models/</guid>
      <description>&lt;p&gt;The language model (LM) is an essential building block of current AI systems dealing with natural language, and has proven to be useful in tasks as diverse as sentiment analysis, text generation, translations, and summarizing. These LMs are typically based on deep neural networks and trained on vast amounts of training data, which makes these so effective, but this doesn&amp;rsquo;t come without any problems.&lt;/p&gt;
&lt;p&gt;LMs have been shown to learn undesirable biases towards certain
social groups, which may unfairly influence the decisions, recommendations or texts
that AI systems building on those LMs generate.
However, the black-box nature of deep neural networks and the fact that these are trained on very large datasets makes it difficult to understand how LMs are biased. Nevertheless, researchers have proposed many different techniques to study the biases that are present in current natural language technologies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this blog post, we will give some examples of techniques we use in our lab to measure one well-studied type of bias, gender bias, in the different representations of language models.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-are-language-models&#34;&gt;What are language models?&lt;/h2&gt;
&lt;p&gt;A language model (LM) is a statistical model to predict the next likely word given a sequence of other words. The most successful ones are based on deep neural networks. There exist many different types of networks (e.g. LSTM, Transformer), but all rely on encoding words or word parts as vector representations and passing these through the &amp;ldquo;hidden layers&amp;rdquo; of the network resulting in &amp;ldquo;contextual embeddings&amp;rdquo; (called such because contextual information is represented in the vectors, such as information on the neighbouring words). Typically, LMs have billions of parameters that are trained on huge datasets, which makes these so effective but also very complex systems.&lt;/p&gt;
&lt;p&gt;While we have a lot of examples showing that LMs exhibit undesirable gender biases, it remains challenging to measure gender bias in these models. In this post, we will focus on three stages in a typical LM pipeline, as highlighted in the figure below, and give examples of how you can measure gender bias in these different representations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/c685cc03a9e1997cf52e804659c48055.png&#34; alt=&#34;c685cc03a9e1997cf52e804659c48055.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;First, we discuss how gender bias can be measured in the training dataset of a model (I) and at the end of the pipeline in a downstream task that makes use of the contextual embeddings of the model (III). We then give an example of how to study gender bias in the internal state of the LM, by studying the input embeddings (IE), wich make up the first layer of the LM (II). However, before we discuss these methods, we give a brief explanation of what we mean by gender bias.&lt;/p&gt;
&lt;h2 id=&#34;gender-bias-of-occupation-terms&#34;&gt;Gender bias of occupation terms&lt;/h2&gt;
&lt;p&gt;Defining (undesirable) bias is a complicated matter, and there are many definitions and frameworks discussed in the literature [e.g. &lt;a href=&#34;https://aclanthology.org/P19-1159.pdf&#34; title=&#34;T. Sun et al. “Mitigating Gender Bias in Natural Language Processing: Literature Review”. 2019.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/2020.acl-main.468/&#34; title=&#34;D. Shah et al. “Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview”. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/2020.acl-main.485/&#34; title=&#34;S. L. Blodgett et al. “Language (Technology) is Power: A Critical Survey of ”Bias” in NLP”. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3&lt;/a&gt;]. In this post, we use &lt;em&gt;gender-neutrality&lt;/em&gt; as the norm, and define bias as any significant deviation from a 50-50% distribution in preferences, probabilities or similarities.&lt;/p&gt;
&lt;p&gt;Inspired by previous work [&lt;a href=&#34;https://www.science.org/doi/abs/10.1126/science.aal4230&#34; title=&#34;A. Caliskan et al. “Semantics derived automatically from language corpora contain human-like biases”. 2017.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;4&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/N18-2002.pdf&#34; title=&#34;R. Rudinger et al. “Gender Bias in Coreference Resolution”. 2018.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;5&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.06032&#34; title=&#34;K. Webster et al. Measuring and Reducing Gendered Correlations in Pre-Trained Models. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;6&lt;/a&gt;], we consider the gender bias of occupation terms in the examples. To quantify the gender bias of an occupation, we often use unambiguously gendered word-pairs (e.g. “man”-“woman”, “he”-“she”, “king”-“queen”) in our measures. It is important to keep in mind, however, that ‘gender’ is a multifaceted concept, which is much more complicated than a simple male-female dichotomy suggests [&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0891243287001002002&#34; title=&#34;C. West and D. H. Zimmerman. “Doing gender”. 1987&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;7&lt;/a&gt;, &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.3109/09540261.2015.1106446&#34; title=&#34;C. Richards et al. “Non-binary or genderqueer genders”. 2016.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;8&lt;/a&gt;].&lt;/p&gt;
&lt;h2 id=&#34;dataset-bias&#34;&gt;Dataset bias&lt;/h2&gt;
&lt;p&gt;It is not a strange idea to investigate the gender bias in the training data of the LM. In the end, the model learns these biases from the data in some way or another. A typical approach to quantify the dataset bias, is to use measurable features in the dataset, such as word counts or how often gendered words co-occur with the words of interest [&lt;a href=&#34;https://ojs.aaai.org/index.php/ICWSM/article/view/14744&#34; title=&#34;E. Fast et al. Shirtless and dangerous: Quantifying linguistic signals of gender bias in an online fiction writing community. 2016.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10&lt;/a&gt;, &lt;a href=&#34;https://pile.eleuther.ai/paper.pdf&#34; title=&#34;L. Gao et al. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;11&lt;/a&gt;, &lt;a href=&#34;https://proceedings.neurips.cc/paper/2019/file/201d546992726352471cfea6b0df0a48-Paper.pdf&#34; title=&#34;Y.C. Tan et al. Assessing Social and Intersectional Biases in Contextualized Word Representations. 2019.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;12&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/N19-1064.pdf&#34; title=&#34;J. Zhao et al. Gender Bias in Contextualized Word Embeddings. 2019.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;13&lt;/a&gt;]. While these statistics can give some indication of possible sources of bias, it doesn&amp;rsquo;t tell use about more nuanced and implicit ways that gender bias can be present in texts that may still be picked up by LMs. Other researchers use special trained classifiers for showing gender bias in texts [&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3287560.3287572&#34; title=&#34;M. De-Artega et al. Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting. 2019.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;14&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/2020.emnlp-main.23.pdf&#34; title=&#34;E. Dinan et al. Multi-Dimensional Gender Bias Classification. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;15&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/2020.emnlp-main.44.pdf&#34; title=&#34;A. Field. Unsupervised discovery of implicit gender bias. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;16&lt;/a&gt;]. However, how these biases are learnt from texts by LMs and what features are important is still an active area of research.&lt;/p&gt;
&lt;h2 id=&#34;downstream-bias&#34;&gt;Downstream bias&lt;/h2&gt;
&lt;p&gt;Most work on the bias of LMs is focused on the output. Any bias in the predictions, recommendations, and texts generated in the downstream task has the potential to actually harm people by unfair behaviour &lt;a href=&#34;https://aclanthology.org/2020.acl-main.485/&#34; title=&#34;S.L. Blodgett et al. Language (technology) is power: A critical survey of \&amp;#34;bias\&amp;#34; in nlp. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[17]&lt;/a&gt;. This bias is typically tested by &lt;em&gt;challenge sets&lt;/em&gt;: carefully constructed sets of sentences used to probe the model for any specific biases.&lt;/p&gt;
&lt;p&gt;One example of a challenge set is the &lt;em&gt;semantic textual similarity task for bias&lt;/em&gt; (&lt;strong&gt;STS-B&lt;/strong&gt;) &lt;a href=&#34;https://arxiv.org/abs/2010.06032&#34; title=&#34;K. Webster et al. Measuring and Reducing Gendered Correlations in Pre-Trained Models. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[18]&lt;/a&gt;. In this task, the LM is used for estimating the similarity of three sentences, containing either the word &amp;ldquo;man&amp;rdquo;, &amp;ldquo;woman&amp;rdquo;, or an occupation term. Then the gender bias for that occupation term is the difference in the similarity averaged over a set of template sentences. Below you see an example for finding the gender bias for &amp;ldquo;janitor&amp;rdquo; using one template sentence, where the final score is 0.75 - 0.54 = 0.21 (male bias).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/STS-B.png&#34; alt=&#34;STS-B.png&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;embedding-bias&#34;&gt;Embedding bias&lt;/h2&gt;
&lt;p&gt;We have seen some examples of measuring gender bias in the training data of an LM and at its output in a downstream task. Something that is not studied as much, is the gender bias in the internal states of an LM. Because of the black-box nature of deep neural networks this is notoriously difficult.&lt;/p&gt;
&lt;p&gt;However, there is one layer in a typical LM that is actually very suitable for measuring bias: the input embeddings. The input embeddings are the first layer of the LM and encode word (parts) into word vectors, which can be used in the other layers. The input embeddings actually have a similar representation to &lt;em&gt;static word embeddings&lt;/em&gt;, for which researchers have actually developed many techniques to measure gender bias. One class of bias measures relies on finding a linear &lt;strong&gt;gender subspace&lt;/strong&gt;. Let us explain one of these methods from Ravfogel et al. &lt;a href=&#34;https://aclanthology.org/2020.acl-main.647/&#34; title=&#34;S. Ravfogel et al. “Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection”. 2020.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[19]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For this measure, we start with a set of unambiguously female and male words (e.g. &amp;ldquo;man&amp;rdquo;-&amp;ldquo;woman&amp;rdquo;, &amp;ldquo;he&amp;rdquo;-&amp;ldquo;she&amp;rdquo;, &amp;ldquo;son&amp;rdquo;-&amp;ldquo;daughter&amp;rdquo;) and train a linear classifier, such as a &lt;em&gt;support vector machine&lt;/em&gt; (SVM), to predict the gender of each word.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/embedding_space_decision_boundary.png&#34; alt=&#34;embedding_space_decision_boundary.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We then define the &lt;strong&gt;gender subspace&lt;/strong&gt; as the axis orthogonal to the decision boundary of the linear SVM that is trained to predict the gender for a set of male and female words.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/embedding_space_gender_subspace.png&#34; alt=&#34;embedding_space_gender_subspace.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we can use the gender subspace for quantifying the gender bias of, for example, occupation terms. For finding the bias score of a word in the embedding space, we use the scalar projection on the gender subspace. In the example below, words like &amp;ldquo;engineer&amp;rdquo; and &amp;ldquo;scientist&amp;rdquo; are on the &amp;ldquo;male&amp;rdquo; side of the subspace (to the left of the purple decision boundary), while &amp;ldquo;nurse&amp;rdquo; and &amp;ldquo;receptionist&amp;rdquo; have a female bias (to the right). The farther away from the decision boundary the word is, the higher the bias.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/embedding_space_occupation_bias.png&#34; alt=&#34;embedding_space_occupation_bias.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let us give an example of what we find if we apply this bias measure to the input embeddings of a Dutch LM called BERTje &lt;a href=&#34;https://arxiv.org/abs/1912.09582&#34; title=&#34;W. de Vries et al. BERTje: A Dutch BERT Model. 2019.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[20]&lt;/a&gt;. When studying the gender bias for Dutch occupation terms, we find clear gender stereotypes in the top 5 male and female biased occupations, as can be seen in the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Female bias (top 5)&lt;/th&gt;
&lt;th&gt;Male bias (top 5)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;kinderopvang (child care)&lt;/td&gt;
&lt;td&gt;ingenieur (engineer)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;verpleegkundige (nurse)&lt;/td&gt;
&lt;td&gt;chauffeur&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;verzorger (caretaker)&lt;/td&gt;
&lt;td&gt;kunstenaar (artist)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;administratie (administration)&lt;/td&gt;
&lt;td&gt;auteur (author)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;schoonheidsspecialist (beauty specialist)&lt;/td&gt;
&lt;td&gt;bouwvakker (construction worker)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we wanted to give some examples of how you can measure the gender bias of a language model. We showed some methods for three different representations in the language modelling pipeline. While it is difficult to measure the gender bias in the internal states of LMs, because of their &amp;ldquo;black box&amp;rdquo; nature, it is actually possible to use existing bias measures on one of the layers: &lt;em&gt;the input embeddings&lt;/em&gt;, for which we also give some examples.&lt;/p&gt;
&lt;p&gt;However, it is important to keep in mind that the research on how to measure bias in LMs is ongoing. Moreover, we also still need more research on how the different bias measures relate to each other to form a good understanding of the underlying mechanisms of bias and the validity of these measures.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
