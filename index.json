[{"authors":["Jim Buissink"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ca291740b74c42f2d65cf6859312c836","permalink":"/author/jim-buissink/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jim-buissink/","section":"authors","summary":"","tags":null,"title":"Jim Buissink","type":"authors"},{"authors":["Katrin Schulz"],"categories":null,"content":"Find out more about Katrin\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"1d161cb0cea26b8392abdafca4288dac","permalink":"/author/katrin-schulz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/katrin-schulz/","section":"authors","summary":"Find out more about Katrin","tags":null,"title":"Katrin Schulz","type":"authors"},{"authors":["Leendert van Maanen"],"categories":null,"content":"Find out more about Leendert\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"39e938dbc71efd9025dae53d06991d7b","permalink":"/author/leendert-van-maanen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/leendert-van-maanen/","section":"authors","summary":"Find out more about Leendert","tags":null,"title":"Leendert van Maanen","type":"authors"},{"authors":["Oskar van der Wal"],"categories":null,"content":"Find out more about Oskar\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b9fa256b7c66a3fab8d823345450c8b4","permalink":"/author/oskar-van-der-wal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/oskar-van-der-wal/","section":"authors","summary":"Find out more about Oskar","tags":null,"title":"Oskar van der Wal","type":"authors"},{"authors":["Sally Hogenboom"],"categories":null,"content":"Find out more about Sally\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"79a78e338fb428050984982bf97ee3e6","permalink":"/author/sally-hogenboom/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/sally-hogenboom/","section":"authors","summary":"Find out more about Sally","tags":null,"title":"Sally Hogenboom","type":"authors"},{"authors":["Wanted_Users"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ef24046a0823d0e69f9de3b5e2b4b4b3","permalink":"/author/wanted/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wanted/","section":"authors","summary":"","tags":null,"title":"Wanted","type":"authors"},{"authors":["Jelle Zuidema"],"categories":null,"content":"Find out more about Willem\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"88e4abf838825368608c6dd0669010de","permalink":"/author/willem-zuidema/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/willem-zuidema/","section":"authors","summary":"Find out more about Willem","tags":null,"title":"Willem Zuidema","type":"authors"},{"authors":[],"categories":[],"content":"Artificial Intelligence (AI) is a technology that is here to stay. In many aspects of daily life, one interacts with AI algorithms in one way or another. These algorithms range from search engines like Google, that present search results in a way that hopefully is optimal to you, to health apps that aim to support your healthy living habits. Because of your omnipresent interactions with AI, it is important to understand how AI systems make decisions. This is especially true, since we need to be able to trust those decisions to be fair and unbiased.\nA clear case where there is a danger for unfairness and bias in AI systems is in the use of large language models. Roughly, these models aim to capture the meaning of natural language expressions; They are used in many domains, including automatic translation, speech recognition, and chat bots. Obviously, it is crucial that these models get the meaning right, and do not introduce unfair biases in the meaning of a phrase!\nWhat makes a bias unfair? The way that most language models express the meaning of the word \u0026ldquo;nurse\u0026rdquo; is highly associated with the way feminine terms (woman, girl, she, etc) are expressed. This does not seem to be what we want, since a male nurse has the same job characteristics as a female nurse. However, this bias, and similar gender biases related to professions, does express the gender gap. In fact, the size of the gender gap in terms of labor statistics over the years has been shown to predict the bias in language over the years. Even stronger, implicit biases that humans seem to have about (among other things) gender, are reflected in language models as well. It seems thus that to some extent language models reflect societal trends, and nothing else. Should we hold these AI algorithms to a higher standard than we do humans? Or should we strive to eradicate bias from language, and consequently from human vocabulary and thought? To answer these questions, an understanding of human bias is necessary as well.\nUnderstanding human biases comes with its own set of problems. Some people may be more biased than others, but how do we measure such a difference? First, it is important to realize that the question implies a golden standard. There could be people out there that are completely unbiased! But what would their behavior and decisions look like? This is a normative question, that society has to address. Second, in order to measure a difference, there need to be good measurement tools. Contrary to many physical properties such as length or weight, there is no precise measurement tool for human bias.\nOne of the reasons may be that human biases and attitudes are inherently personal, and it is extremely difficult to develop tests that allow for the heterogeneity of biases that are induced by images, words, or names. For example, we are currently running a study to show that biases that have been reported in past studies (e.g., towards women) may depend on the use of specific words (as in the image below), rather than show an implicit bias towards women per se.\nIn the Gender-Career Implicit Association Test, people have to press a right or a left button in response to either gender names or two supposedly gender-stereotypical categories (Career and Family). The idea is that this is harder when there is a \u0026ldquo;mismatch\u0026rdquo; between the category and the gender that require the same button press.\nThis is the challenge our research group stands for: To understand how bias in large language models relates to bias in humans. We thus need to address fundamental questions about the nature and measurement of such biases. A daunting challenge for sure, but the end goal is extremely worthwhile: To mitigate unfair biases in the AI systems we interact with on a daily basis, such that we can gain trust in their decisions, and fairness is achieved.\n","date":1643846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643846400,"objectID":"686cec2a33b67f2f7439858605ec7d26","permalink":"/blogs_posts/understanding-relation-language-model-and-human-bias/","publishdate":"2022-02-03T00:00:00Z","relpermalink":"/blogs_posts/understanding-relation-language-model-and-human-bias/","section":"blogs_posts","summary":"Artificial Intelligence (AI) is a technology that is here to stay. In many aspects of daily life, one interacts with AI algorithms in one way or another. These algorithms range from search engines like Google, that present search results in a way that hopefully is optimal to you, to health apps that aim to support your healthy living habits.","tags":["gender bias"],"title":"The difficult problem of understanding the relation between bias in language models and human biases","type":"blogs_posts"},{"authors":[],"categories":[],"content":"The language model (LM) is an essential building block of current AI systems dealing with natural language, and has proven to be useful in tasks as diverse as sentiment analysis, text generation, translations, and summarizing. These LMs are typically based on deep neural networks and trained on vast amounts of training data, which makes these so effective, but this doesn\u0026rsquo;t come without any problems.\nLMs have been shown to learn undesirable biases towards certain social groups, which may unfairly influence the decisions, recommendations or texts that AI systems building on those LMs generate. However, the black-box nature of deep neural networks and the fact that these are trained on very large datasets makes it difficult to understand how LMs are biased. Nevertheless, researchers have proposed many different techniques to study the biases that are present in current natural language technologies.\nIn this blog post, we will give some examples of techniques we use in our lab to measure one well-studied type of bias, gender bias, in the different representations of language models.\nWhat are language models? A language model (LM) is a statistical model to predict the next likely word given a sequence of other words. The most successful ones are based on deep neural networks. There exist many different types of networks (e.g. LSTM, Transformer), but all rely on encoding words or word parts as vector representations and passing these through the \u0026ldquo;hidden layers\u0026rdquo; of the network resulting in \u0026ldquo;contextual embeddings\u0026rdquo; (called such because contextual information is represented in the vectors, such as information on the neighbouring words). Typically, LMs have billions of parameters that are trained on huge datasets, which makes these so effective but also very complex systems.\nWhile we have a lot of examples showing that LMs exhibit undesirable gender biases, it remains challenging to measure gender bias in these models. In this post, we will focus on three stages in a typical LM pipeline, as highlighted in the figure below, and give examples of how you can measure gender bias in these different representations.\nFirst, we discuss how gender bias can be measured in the training dataset of a model (I) and at the end of the pipeline in a downstream task that makes use of the contextual embeddings of the model (III). We then give an example of how to study gender bias in the internal state of the LM, by studying the input embeddings (IE), wich make up the first layer of the LM (II). However, before we discuss these methods, we give a brief explanation of what we mean by gender bias.\nGender bias of occupation terms Defining (undesirable) bias is a complicated matter, and there are many definitions and frameworks discussed in the literature [e.g. 1, 2, 3]. In this post, we use gender-neutrality as the norm, and define bias as any significant deviation from a 50-50% distribution in preferences, probabilities or similarities.\nInspired by previous work [4, 5, 6], we consider the gender bias of occupation terms in the examples. To quantify the gender bias of an occupation, we often use unambiguously gendered word-pairs (e.g. “man”-“woman”, “he”-“she”, “king”-“queen”) in our measures. It is important to keep in mind, however, that ‘gender’ is a multifaceted concept, which is much more complicated than a simple male-female dichotomy suggests [7, 8].\nDataset bias It is not a strange idea to investigate the gender bias in the training data of the LM. In the end, the model learns these biases from the data in some way or another. A typical approach to quantify the dataset bias, is to use measurable features in the dataset, such as word counts or how often gendered words co-occur with the words of interest [10, 11, 12, 13]. While these statistics can give some indication of possible sources of bias, it doesn\u0026rsquo;t tell use about more nuanced and implicit ways that gender bias can be present in texts that may still be picked up by LMs. Other researchers use special trained classifiers for showing gender bias in texts [14, 15, 16]. However, how these biases are learnt from texts by LMs and what features are important is still an active area of research.\nDownstream bias Most work on the bias of LMs is focused on the output. Any bias in the predictions, recommendations, and texts generated in the downstream task has the potential to actually harm people by unfair behaviour [17]. This bias is typically tested by challenge sets: carefully constructed sets of sentences used to probe the model for any specific biases.\nOne example of a challenge set is the semantic textual similarity task for bias (STS-B) [18]. In this task, the LM is used for estimating the similarity of three sentences, containing either the word \u0026ldquo;man\u0026rdquo;, \u0026ldquo;woman\u0026rdquo;, or an occupation term. Then the gender bias for that occupation term is the difference in the similarity averaged over a set of template sentences. Below you see an example for finding the gender bias for \u0026ldquo;janitor\u0026rdquo; using one template sentence, where the final score is 0.75 - 0.54 = 0.21 (male bias).\nEmbedding bias We have seen some examples of measuring gender bias in the training data of an LM and at its output in a downstream task. Something that is not studied as much, is the gender bias in the internal states of an LM. Because of the black-box nature of deep neural networks this is notoriously difficult.\nHowever, there is one layer in a typical LM that is actually very suitable for measuring bias: the input embeddings. The input embeddings are the first layer of the LM and encode word (parts) into word vectors, which can be used in the other layers. The input embeddings actually have a similar representation to static word embeddings, for which researchers have actually developed many techniques to measure gender bias. One class of bias measures relies on finding a linear gender subspace. Let us explain one of these methods from Ravfogel et al. [19].\nFor this measure, we start with a set of unambiguously female and male words (e.g. \u0026ldquo;man\u0026rdquo;-\u0026ldquo;woman\u0026rdquo;, \u0026ldquo;he\u0026rdquo;-\u0026ldquo;she\u0026rdquo;, \u0026ldquo;son\u0026rdquo;-\u0026ldquo;daughter\u0026rdquo;) and train a linear classifier, such as a support vector machine (SVM), to predict the gender of each word.\nWe then define the gender subspace as the axis orthogonal to the decision boundary of the linear SVM that is trained to predict the gender for a set of male and female words.\nNow we can use the gender subspace for quantifying the gender bias of, for example, occupation terms. For finding the bias score of a word in the embedding space, we use the scalar projection on the gender subspace. In the example below, words like \u0026ldquo;engineer\u0026rdquo; and \u0026ldquo;scientist\u0026rdquo; are on the \u0026ldquo;male\u0026rdquo; side of the subspace (to the left of the purple decision boundary), while \u0026ldquo;nurse\u0026rdquo; and \u0026ldquo;receptionist\u0026rdquo; have a female bias (to the right). The farther away from the decision boundary the word is, the higher the bias.\nLet us give an example of what we find if we apply this bias measure to the input embeddings of a Dutch LM called BERTje [20]. When studying the gender bias for Dutch occupation terms, we find clear gender stereotypes in the top 5 male and female biased occupations, as can be seen in the table below.\n   Female bias (top 5) Male bias (top 5)     kinderopvang (child care) ingenieur (engineer)   verpleegkundige (nurse) chauffeur   verzorger (caretaker) kunstenaar (artist)   administratie (administration) auteur (author)   schoonheidsspecialist (beauty specialist) bouwvakker (construction worker)    Conclusion In this blog post, we wanted to give some examples of how you can measure the gender bias of a language model. We showed some methods for three different representations in the language modelling pipeline. While it is difficult to measure the gender bias in the internal states of LMs, because of their \u0026ldquo;black box\u0026rdquo; nature, it is actually possible to use existing bias measures on one of the layers: the input embeddings, for which we also give some examples.\nHowever, it is important to keep in mind that the research on how to measure bias in LMs is ongoing. Moreover, we also still need more research on how the different bias measures relate to each other to form a good understanding of the underlying mechanisms of bias and the validity of these measures.\n","date":1643760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643760000,"objectID":"1c9edf9d53ef407cfb1d0e52537e9c81","permalink":"/blogs_posts/gender-bias-in-language-models/","publishdate":"2022-02-02T00:00:00Z","relpermalink":"/blogs_posts/gender-bias-in-language-models/","section":"blogs_posts","summary":"The language model (LM) is an essential building block of current AI systems dealing with natural language, and has proven to be useful in tasks as diverse as sentiment analysis, text generation, translations, and summarizing.","tags":["gender bias"],"title":"How do I know if my language model is gender-biased?","type":"blogs_posts"},{"authors":[],"categories":[],"content":" Please contact us at: s.a.m.hogenboom [at] uva.nl for a zoom-link to join the seminar.    Date: 26th of January, 2021 Time: 14:00 - 15:00 (Amsterdam, UTC + 01:00)  Summary\n A stereotype is a generalization about a class of people but does not necessarily represent every individual within the group (McCauley, Stitt, \u0026amp; Segal, 1980). Category information (i.e., stereotype information) is often used to make probabilistic predictions about people within a particular group. For instance, a probabilistic judgement about Germans would be that, “Germans are more likely than other people to be efficient.” Here we are making a prediction about an individual’s personality (i.e., efficiency) based on their group membership (i.e., German). McCauley and Stitt (1978) suggest that people are acctualy Bayesian in their judgements and tend to make probabilistic judgements about people’s personality based on stereotype information. The current project aims to replicate the original McCauley and Stitt (1978) work to test whether stereotype prediction from category information to personality adheres to Bayes’ rule.\n Literature\n McCauley, C., \u0026amp; Stitt, C. L. (1978). An individual and quantitative measure of stereotypes. Journal of Personality and Social Psychology, 36(9), 929. pdf  ","date":1611014400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611014400,"objectID":"4b6cae0d9252e540bc6c9427b15ea321","permalink":"/seminars_posts/judgement-of-social-groups/","publishdate":"2021-01-19T00:00:00Z","relpermalink":"/seminars_posts/judgement-of-social-groups/","section":"seminars_posts","summary":"26th of January, 2021 - 14:00 - 15:00 (Amsterdam, UTC + 01:00)","tags":["seminars","open access"],"title":"Judgements of Social Groups","type":"seminars_posts"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"14ee683c4b38c2d729a72d6fc8a7d1f7","permalink":"/blogs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blogs/","section":"","summary":"Here we occasionally share some things about our work.","tags":null,"title":"Blog","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Contact the Team","tags":null,"title":"Contact Page","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"199c1eb19efe4d0257ca4739fdc82f5d","permalink":"/seminars/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/seminars/","section":"","summary":"We invite you to join us for the upcoming seminars","tags":null,"title":"Seminars","type":"widget_page"}]